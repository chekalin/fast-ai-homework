{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOME = '/home/stanislav/Dev/fast-ai-homework/seedling-classification'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_HOME = HOME + '/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "from shutil import copyfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd $DATA_HOME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%rm -rf test/ train/ valid/ results/ sample/ sample_submission.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip test.zip && unzip train.zip && unzip sample_submission.csv.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv('sample_submission.csv')\n",
    "sample_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = os.listdir(DATA_HOME + '/train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for category in categories: \n",
    "    %mkdir -p \"$DATA_HOME/valid/$category\"\n",
    "    %mkdir -p \"$DATA_HOME/sample/valid/$category\"\n",
    "    %mkdir -p \"$DATA_HOME/sample/train/$category\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%mkdir -p \"$DATA_HOME/results\"\n",
    "%mkdir -p \"$DATA_HOME/sample/results\"\n",
    "%mkdir -p \"$DATA_HOME/test/unknown\"\n",
    "%mkdir -p \"$DATA_HOME/sample/test/unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "% cd $DATA_HOME/test\n",
    "!echo *.png | xargs mv -t unknown/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd $DATA_HOME/train\n",
    "training_files = glob('*/*.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_set_size = int(len(training_files) * .3)\n",
    "\n",
    "shuffled_training_files = np.random.permutation(training_files).tolist()\n",
    "for i in range(validation_set_size): \n",
    "    random_file = shuffled_training_files.pop()\n",
    "    os.rename(random_file, DATA_HOME + '/valid/' + random_file)\n",
    "\n",
    "sample_training_set_size = 1000\n",
    "for i in range(sample_training_set_size): \n",
    "    random_file = shuffled_training_files.pop()\n",
    "    copyfile(random_file, DATA_HOME + '/sample/train/' + random_file)\n",
    "\n",
    "sample_validation_set_size = 100\n",
    "for i in range(sample_validation_set_size): \n",
    "    random_file = shuffled_training_files.pop()\n",
    "    copyfile(random_file, DATA_HOME + '/sample/valid/' + random_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd $DATA_HOME/test\n",
    "test_files = glob('*/*.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_test_set_size = 100\n",
    "for filename in test_files[:sample_test_set_size]: copyfile(filename, DATA_HOME + '/sample/test/' + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd $DATA_HOME\n",
    "for dirpath, dirnames, filenames in os.walk(DATA_HOME):\n",
    "    relative = dirpath[len(os.getcwd()):]\n",
    "    if (len(filenames) and len(relative)): print(\"Files in {}: {}\".format(relative, len(filenames)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.Session()\n",
    "\n",
    "from keras import backend as K\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(precision=4, linewidth=100)\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, GlobalAveragePooling2D, Flatten, Dense, Dropout, Conv2D, MaxPooling2D, ZeroPadding2D, BatchNormalization\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.xception import Xception\n",
    "from keras.optimizers import SGD, RMSprop, Adam\n",
    "from keras.utils.data_utils import get_file\n",
    "\n",
    "def addConvBlock(x, layers, filters):\n",
    "    for i in range(layers):\n",
    "        x = Conv2D(filters, (3, 3), padding='same', activation='relu')(x)\n",
    "        x = BatchNormalization(axis=1)(x)\n",
    "    return MaxPooling2D(strides=(2, 2))(x)\n",
    "\n",
    "def add_top(x):\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(4096, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(4096, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    return Dense(12, activation='softmax')(x)\n",
    "\n",
    "def vgg_16_from_scatch():\n",
    "    inputs = Input(shape=(224,224,3))\n",
    "    x = BatchNormalization(axis=1)(inputs)\n",
    "    x = addConvBlock(x, 2, 64)\n",
    "    x = addConvBlock(x, 2, 128)\n",
    "    x = addConvBlock(x, 2, 256)\n",
    "    x = addConvBlock(x, 2, 512)\n",
    "    x = addConvBlock(x, 2, 512)\n",
    "    x = add_top(x)\n",
    "    return Model(inputs, x)\n",
    "    \n",
    "# model = vgg_16_from_scatch()  \n",
    "# vgg16 = VGG16(include_top=False, weights=\"imagenet\", input_shape = (224,224,3))\n",
    "# for layer in vgg16.layers: layer.trainable = False\n",
    "\n",
    "# Xception().summary()\n",
    "xception = Xception(include_top=False, weights=\"imagenet\", input_shape = (299,299,3))\n",
    "# for layer in xception.layers: layer.trainable = False\n",
    "x = xception.output\n",
    "x = GlobalAveragePooling2D(name='avg_pool')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(12, activation='softmax', name='predictions')(x)\n",
    "\n",
    "model = Model(xception.input, x)\n",
    "model.compile(Adam(decay=1e-6), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = DATA_HOME + '/sample'\n",
    "path = DATA_HOME\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "\n",
    "def get_batches(path, gen=image.ImageDataGenerator(), shuffle=True, batch_size=8, class_mode='categorical'):\n",
    "        return gen.flow_from_directory(path, \n",
    "                                       target_size=(299,299),\n",
    "                                       class_mode=class_mode, \n",
    "                                       shuffle=shuffle, \n",
    "                                       batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = image.ImageDataGenerator(rotation_range=180, horizontal_flip=True, vertical_flip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = get_batches(path + '/train', batch_size=batch_size, gen=gen)\n",
    "val_batches = get_batches(path + '/valid', batch_size=batch_size, gen=gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"seedlings-xception.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "modelsave = ModelCheckpoint(filepath='seedlings-xception.h5', save_best_only=True)\n",
    "model.fit_generator(batches, epochs=600, validation_data=val_batches, callbacks=[modelsave])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1425 images belonging to 12 classes.\n"
     ]
    }
   ],
   "source": [
    "val_batches = get_batches(path + '/valid', batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict_generator(val_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_classes = np.argmax(predictions, axis = 1)\n",
    "actual_classes = val_batches.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision score: 0.9796%\n",
      "Recall score: 0.9796%\n",
      "F1: 0.9796%\n",
      "F1 calculated: 0.9796%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, log_loss, recall_score, f1_score\n",
    "\n",
    "precision = precision_score(actual_classes, predicted_classes, average='micro');\n",
    "recall = recall_score(actual_classes, predicted_classes, average='micro')\n",
    "f1 = (2 * precision * recall) / (precision + recall)\n",
    "f1_calculated = f1_score(actual_classes, predicted_classes, average='micro')\n",
    "\n",
    "print('Precision score: {:.4f}%'.format(precision))\n",
    "print('Recall score: {:.4f}%'.format(recall))\n",
    "print('F1: {:.4f}%'.format(f1))\n",
    "print('F1 calculated: {:.4f}%'.format(f1_calculated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1396 correct predictions found\n"
     ]
    }
   ],
   "source": [
    "idxs = np.where(actual_classes == predicted_classes)[0]\n",
    "print('{} correct predictions found'.format(len(idxs)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run prediction and prepare submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 794 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "test_batches = get_batches(path + '/test', batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict_generator(test_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "results_file = path + '/results/submission.csv'\n",
    "file = open(results_file, \"w\")\n",
    "writer = csv.writer(file, delimiter=',', quoting=csv.QUOTE_NONE)\n",
    "header = ['file','species']\n",
    "writer.writerow(header)\n",
    "\n",
    "predicted_classes = np.argmax(predictions, axis = 1)\n",
    "\n",
    "\n",
    "for filename, prediction in zip(test_batches.filenames, predicted_classes):\n",
    "    writer.writerow([filename[len('unknown/'):]] + [sorted(categories)[prediction]])\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/stanislav/Dev/fast-ai-homework/seedling-classification\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href='data/results/submission.csv' target='_blank'>data/results/submission.csv</a><br>"
      ],
      "text/plain": [
       "/home/stanislav/Dev/fast-ai-homework/seedling-classification/data/results/submission.csv"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import FileLink\n",
    "%cd $HOME\n",
    "relative_path_to_results = results_file.replace(HOME, '')[1:]\n",
    "FileLink(relative_path_to_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
